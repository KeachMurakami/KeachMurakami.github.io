---
title: "論文タイトルのテキスト解析 (RMeCab)"
output: html_document
layout: post
tags: lab R textmining RMeCab
---

[昨日のコード](https://keachmurakami.github.io/2016/12/25/TitlesScraping.html)を転用して、論文タイトルでテキスト解析  

- タイトル中での出現頻度が高い語句は？
- タイトル中での出現頻度が増えている語句は？
- 各誌で掲載される論文の傾向は？

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse) # ggplotとかdplyrとか
library(rvest) # webスクレイピング用
library(XML) # rvestで必要なので
library(magrittr)
library(stringr) # 文字列操作用
library(data.table)
library(RMeCab)
library(wordcloud2)
library(knitr)
library(broom)
library(foreach)
library(pforeach)

# 雑誌リスト
NP <- "1469-8137" #New Phytologist
PCE <- "1365-3040"  # Plant, Cell & Environment
PJ <- "1365-313X" # The Plant Journal
PP <- "1399-3054" # Physiologia Plantarum
PB <- "1438-8677" # Plant Biology

# 作業用フォルダを作って移動
setwd("/Users/keach/Dropbox/KeachMurakami.github.io/_source/")
dir.create("2016-12-26")
setwd("2016-12-26")
```

```{r echo = F}
extract_titles <-
  function(issue_url){
    html_issue <- 
      issue_url %>%
      read_html
    
    # 雑誌名、出版年月、巻号を取得
    journal <-
      html_issue %>%
      html_nodes("h1#productTitle") %>% # タグがh1, IDがproductTitle
      html_text
    date <-
      html_issue %>%
      html_nodes("h2.noMargin") %>% # タグがh2, クラスがnoMargin
      html_text
    issue_volume <-
      html_issue %>%
      html_nodes("p.issueAndVolume") %>% # タグがp, クラスがissueAndVolume
      html_text
    
    # タイトル一覧を抽出
    titles_issue <-
      html_issue %>%
      # タグがdiv, クラスがcitation.tocArticle以下のaタグのついた要素を抜き出す
      html_nodes("div.citation.tocArticle a") %>%
      html_text %>%
      data_frame(title = .) %>%
      # いくつか余計な要素が含まれているので、文字列の一致不一致で抽出箇所を限定する
      filter(str_detect(title, "pages")) %>% # extract titles
      filter(!str_detect(title, "Issue Information")) %>% # exclude issue info
      mutate(journal = journal,
             date = date,
             issue_volume = issue_volume)

    # 該当Issueの１号前のIssueのTOCのURLを取得する
    previous_url <-
      html_issue %>%
      html_nodes("a.previous") %>% # タグがa, クラスがprevious
      html_attr("href") %>% # href (URLリンク) 要素を抽出
      paste0("http://onlinelibrary.wiley.com/", .)
    
    return(list(titles_issue, previous_url))
  }
extract_ISSN <-
  function(ISSN, number_parse = 10){
    # get Current issue URL from ISSN
    current_url <-
      paste0("http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)", ISSN, "/issues") %>%
      read_html %>%
      html_nodes("a#currentIssueLink") %>%
      html_attr("href") %>%
      paste0("http://onlinelibrary.wiley.com/", .)

    lapply(1:number_parse, function(i){
      extract_titles(current_url) %>%
        {
          current_url <<- .[[2]] # 親環境で結果を保存
          cat("ISSN", ISSN, "; finish: volume", i, "\n", sep = "")
          .[[1]] %>% return
        }
    })
  }
```


### 解析

- 対象は前回同様、Wileyの植物系の5誌
    - 各誌100号分のタイトルを取得
        - 雑誌によって発行頻度が違うので、解析期間も違う
        - 古いものだと1998年あたりのデータから解析
        - 途中で雑誌名が代わっているものは、変更前を省略
- 日本語形態素解析器[MeCab<sup>1</sup>](http://taku910.github.io/mecab/)をR経由 ([RMeCab<sup>2</sup>](https://sites.google.com/site/rmecab/)経由) で動かして、テキストマイニング  

```{r eval = F}
# 並列計算で5誌の100号をスクレイプ
title_list <-
  pforeach(i = c(NP, PCE, PJ, PP, PB), .combine = bind_rows)({
    extract_ISSN(ISSN = NP, number_parse = 100)
  })

title_df <-
  title_list %>%
  tidyr::extract(col = title, into = c("title", "start", "end"), regex = "(.+)\\(pages(.+[0-9]+)–([0-9]+)") %>% # remove pages
  tidyr::extract(col = issue_volume, into = c("vol", "issue"), regex = "([0-9]+).+([0-9]+)") %>% # separate volume and issue
  tidyr::extract(col = date, into = c("month", "year"), regex = "([a-zA-Z]+).([0-9]+)") # separate month and year

# csvで保存
title_df %>%
  write_csv(path = "./titles_scrape.csv")  
}
```

- [`pforeach`<sup>3</sup>](http://d.hatena.ne.jp/hoxo_m/20141222/p1)による並列計算が簡単で素晴らしい
    - scrape処理の並列化で実行時間がざっくり1/2

```{r eval = F}
# 非並列計算
system.time(
  lapply(
    c(NP, PCE, PJ, PP, PB), function(i){
      extract_ISSN(ISSN = i, number_parse = 5)
      })
  )

 #   user  system elapsed 
 # 10.365  38.943  53.938 


# 4コア並列計算
system.time(
  pforeach::pforeach(i = c(NP, PCE, PJ, PP, PB), .combine = bind_rows)({
    extract_ISSN(ISSN = i, number_parse = 5)
    })
  )

  #  user  system elapsed 
  # 5.508  15.315  24.520 
```

### テキストマイニング

- 品詞で解析とか、ステム (photosynthesis と photosynthetic のような関係性) の解析とか、気になるけど放置
- 英語タイトルの解析だからそもそもMeCab (日本語用) を使うのが愚かだった
- MeCabがファイルからしか解析できないので、一時ファイル出力、という感じ

```{r message = FALSE, prompt= FALSE}
titles <-
  fread("./titles_scrape.csv") %>%
  as_data_frame %>%
  filter(!(journal %in% c("Acta Botanica Neerlandica", "Botanica Acta"))) %>% # 雑誌名の変更前のものを除く
  mutate(title = tolower(title)) # 全て小文字に

# 品詞で解析したかったけど...
RemoveList <-
  c("of", "in", "on", "to", "for", "by", "with", "from",
    "at", "among", "but", "or", "into", "under", "via", "during",
    "and", "is", "are", "as", "between", "that", "its", "their", 
    "the", "a", "an")

titles_grouped <-
  titles %>%
  group_by(journal, year) %>%
  do(
    .$title %>%
    paste(collapse = " ") %>%
    data_frame(title = .)
  )

titles_freqs <-
  foreach(i = 1:dim(titles_grouped)[1], .combine = bind_rows) %do%
  {
    df_slice <-
      titles_grouped[i, ]

    # MeCabがファイルからしか解析できないので、一時ファイルを吐く
    df_slice %$%
      title %>%
      write.table(file = "temp.txt", row.names = F, col.names = F)
    
    # MeCabる
    journal <- df_slice$journal
    year <- df_slice$year
    
    # 整える
    result_mecab <-
      "temp.txt" %>%
      RMeCabFreq %>%
      filter(Info1 != "記号", Info2 == "一般", !(Term %in% RemoveList)) %>%
      mutate(journal = journal, year = year) %>%
      select(term = Term, freq = Freq, journal, year) %>%
      return
  }

# 解析結果をcsvで保存
titles_freqs %>%
  write_csv(path = "titles_analyzed.csv")
```


### データ解析・可視化

#### 全体

- 解析期間を通しての出現回数のトップ30までの単語を表示、全単語をWordCloud化
    - Arabidopsis thaliana (シロイヌナズナ) がつよい
        - その他の固有名詞だと、riceくらい
    - coはたぶんCO<sub>2</sub>
    - lってなんだ、と思ったら、種名をつけたLinnéのL
    - WordCloudはおしゃれだけど得られるものは少ない

```{r graphs, warning = FALSE}
freqs_all <-
  titles_freqs %>%
  group_by(term) %>%
  summarise(freq = sum(freq))

# 出現総数TOP30
freqs_all %>%
  arrange(desc(freq)) %>%
  head(30) %>%
  ggplot(aes(x = reorder(term, freq), y = freq)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "word")

# ワードクラウド
freqs_all %>%
  arrange(desc(freq)) %>%
  wordcloud2(size = .3, color = "lightgreen")
```

#### 計時推移

- 頻出30単語を抽出して、計時推移をプロット
    - 総単語数  (上図) が増えているため、割合にして確認 (下図)

```{r}
major_terms <-
  freqs_all %>%
  arrange(desc(freq)) %>%
  head(30) %$%
  term

# 年ごとの出現word総数
terms_per_year <-
  titles_freqs %>%
  filter(term %in% major_terms) %>%
  group_by(year) %>%
  summarise(total = sum(freq))

time_dat <-
  titles_freqs %>%
  filter(term %in% major_terms) %>%
  group_by(year, term) %>%
  summarise(freq = sum(freq)) %>%
  left_join(., terms_per_year, by = "year") %>%
  filter(year != "2017") %>%  # 2017年はデータがまだ少ないので除外
  mutate(freq_percent = freq / total) %>%
  gather(category, freq, -year, -term) %>%
  filter(category != "total")

# 経年推移
time_dat %>%
  ggplot(aes(x = year, y = freq, col = term)) +
  geom_line() +
  geom_point() +
  facet_grid(category ~ ., scale = "free")
```
    
- Arabidopsisの時代は過ぎた？
    - 転換期 (2011年) 以降の推移を線形回帰 (雑) すると、genes・expressionも同時期に低下傾向
    - 時代はstress応答 (stress・water・tolerance)
        - 雑な解析ではあるが、感覚には即している
        - この気運を状態空間モデルなんかで表現できると面白い

```{r}
# 線形回帰でごり押し
time_dat %>%
  filter(year > 2010, category == "freq_percent") %>%
  rename(Term = term) %>%
  group_by(Term) %>%
  do(lm(data = ., formula = freq ~ year) %>% tidy) %>%
  filter(term == "year") %>%
  select(term = Term, increase = estimate, p = p.value) %>%
  mutate(signif = if_else(p < 0.05, "*", "")) %>%
  arrange(desc(increase)) %>%
  kable

# いくつの単語の最近のトレンドに注目
time_dat %>%
  filter(term %in% c("genes", "expression", "tolerance", "water", "stress", "light", "temperature"),
         year > 2010) %>%
  ggplot(aes(x = year, y = freq, col = term)) +
  geom_line() +
  geom_point() +
  facet_grid(category ~ ., scale = "free")
```

<br>

#### 雑誌ごとの色

- 各誌の頻出単語を30位まで
    - New Phytologistは他と毛色が違う
        - 菌類 (fungal・mycorrhizal) 系の話が多い
            - いわれてみると、菌との相互関係だとかbiome系の論文は最近よくみかける気がする
            - が、まったく読んでない
            - たまには読んでみよう
        - root・soil・evolution・forestとスケールがでかい
    - Physiologia PlantarumとPCEは環境応答系に注力している
        - tolerance・drought・water・light・CO<sub>2</sub>
        - 光合成関係だとやっぱりこのふたつ
    - The Plant Journalは完全にMolecularな感じ
        - 個人的にはあまり読んでも楽しくない (分子細胞生物学に弱い) ので敬遠しがち
    - Plant Biologyは傾向がとりづらい
        - 良く言えば間口が広い、悪く言えば軸がぶれている
        - 花粉系...? (pollen, pollination)

```{r results = 'asis'}
titles_freqs %>%
  group_by(journal, term) %>%
  summarise(freq = sum(freq)) %>%
  do(arrange(., desc(freq)) %>%
       head(30) %>%
       mutate(rank = 1:30)) %>%
  ungroup %>%
  transmute(journal, term = paste0(term, " (", freq, ")"), rank) %>%
  spread(journal, term) %>%
  kable
```

<br>

#### 参考ページ
[<sup>1</sup>MeCab](http://taku910.github.io/mecab/)  
[<sup>2</sup>RMeCab](https://sites.google.com/site/rmecab/)  
[<sup>3</sup>Rで超簡単に並列処理を書けるパッケージ pforeach を作った](http://d.hatena.ne.jp/hoxo_m/20141222/p1)  